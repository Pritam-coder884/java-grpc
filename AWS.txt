Cloud Computing :
---------------------

# changes 1


On-demand delivery of IT resources and applications through the internet with pay-as-you-go pricing.


Cloud computing deployment models :

1. Cloud Based Deployments :
------------------------------
  - Run all parts of the application in the cloud.

  - Migrate existing applications to the cloud.

  - Design and build new applications in the cloud.

In a cloud-based deployment model, you can migrate existing applications to the cloud, or you can design and build new applications in the cloud. You can build those applications on low-level infrastructure that requires your IT staff to manage them. Alternatively, you can build them using higher-level services that reduce the management, architecting, and scaling requirements of the core infrastructure.

For example, a company might create an application consisting of virtual servers, databases, and networking components that are fully based in the cloud. 



2. On-premises Deployment :
-----------------------------
- Deploy resources by using virtualization and resource management tools.

- Increase resource utilization by using application management and virtualization technologies.

- On-premises deployment is also known as a private cloud deployment. In this model, resources are deployed on premises by using virtualization and resource management tools.


For example, you might have applications that run on technology that is fully kept in your on-premises data center. Though this model is much like legacy IT infrastructure, its incorporation of application management and virtualization technologies helps to increase resource utilization.


3. Hybrid Deployment :
-----------------------
 - Connect cloud-based resources to on-premises infrastructure.

 - Integrate cloud-based resources with legacy IT applications.

 In a hybrid deployment, cloud-based resources are connected to on-premises infrastructure. You might want to use this approach in a number of situations. For example, you have legacy applications that are better maintained on premises, or government regulations require your business to keep certain records on premises.


For example, suppose that a company wants to use cloud services that can automate batch data processing and analytics. However, the company has several legacy applications that are more suitable on premises and will not be migrated to the cloud. With a hybrid deployment, the company would be able to keep the legacy applications on premises while benefiting from the data and analytics services that run in the cloud.







Amazon Elastic Compute Cloud (Amazon EC2):
-------------------------------------------
Amazon Elastic Compute Cloud (Amazon EC2)(opens in a new tab) provides secure, resizable compute capacity in the cloud as Amazon EC2 instances. 


To learn more about Amazon EC2 instance types, expand each of the following five categories.


1. General purpose instances :
----------------------------
General purpose instances provide a balance of compute, memory, and networking resources. You can use them for a variety of workloads, such as:
- application servers
- gaming servers
- backend servers for enterprise applications
- small and medium databases
Suppose that you have an application in which the resource needs for compute, memory, and networking are roughly equivalent. You might consider running it on a general purpose instance because the application does not require optimization in any single resource area.





2. Compute optimized instances :
-----------------------------
Compute optimized instances are ideal for compute-bound applications that benefit from high-performance processors. Like general purpose instances, you can use compute optimized instances for workloads such as web, application, and gaming servers.

However, the difference is compute optimized applications are ideal for high-performance web servers, compute-intensive applications servers, and dedicated gaming servers. You can also use compute optimized instances for batch processing workloads that require processing many transactions in a single group.

It is used Amazon EC2 instance for a batch processing workload.



3. Memory optimized instances :
-----------------------------
Memory optimized instances are designed to deliver fast performance for workloads that process large datasets in memory. In computing, memory is a temporary storage area. It holds all the data and instructions that a central processing unit (CPU) needs to be able to complete actions. Before a computer program or application is able to run, it is loaded from storage into memory. This preloading process gives the CPU direct access to the computer program.

Suppose that you have a workload that requires large amounts of data to be preloaded before running an application. This scenario might be a high-performance database or a workload that involves performing real-time processing of a large amount of unstructured data. In these types of use cases, consider using a memory optimized instance. Memory optimized instances enable you to run workloads with high memory needs and receive great performance.


4. Accelerated computing instances :
-------------------------------------
Accelerated computing instances use hardware accelerators, or coprocessors, to perform some functions more efficiently than is possible in software running on CPUs. Examples of these functions include floating-point number calculations, graphics processing, and data pattern matching.

In computing, a hardware accelerator is a component that can expedite data processing. Accelerated computing instances are ideal for workloads such as graphics applications, game streaming, and application streaming.


5. Storage optimized instances :
-------------------------------
Storage optimized instances are designed for workloads that require high, sequential read and write access to large datasets on local storage. Examples of workloads suitable for storage optimized instances include distributed file systems, data warehousing applications, and high-frequency online transaction processing (OLTP) systems.

In computing, the term input/output operations per second (IOPS) is a metric that measures the performance of a storage device. It indicates how many different input or output operations a device can perform in one second. Storage optimized instances are designed to deliver tens of thousands of low-latency, random IOPS to applications. 

You can think of input operations as data put into a system, such as records entered into a database. An output operation is data generated by a server. An example of output might be the analytics performed on the records in a database. If you have an application that has a high IOPS requirement, a storage optimized instance can provide better performance over other instance types not optimized for this kind of use case.





1. which Amazon EC2 instance type offers high-performance processors? 

ans. Compute optimized

2. Which Amazon EC2 instance type is ideal for high-performance databases?

ans. Memory optimized

3. Which Amazon EC2 instance type balances compute, memory, and networking resources?

ans. General purpose

4. Which Amazon EC2 instance type is suitable for data warehousing applications?

ans. Storage Optimized

5. Which Amazon EC2 pricing option provides a discount when you specify a number of EC2 instances to run a specific OS, instance family and size, and tenancy in one Region?

ans. Standard Reserved Instances

6. Which Amazon EC2 pricing option provides a discount when you make an hourly spend commitment to an instance family and Region for a 1-year or 3-year term?

ans. EC2 Instance Savings Plans




Scalability :
-------------
Scalability involves beginning with only the resources you need and designing your architecture to automatically respond to changing demand by scaling out or in. As a result, you pay for only the resources you use. You don’t have to worry about a lack of computing capacity to meet your customers’ needs.



Amazon EC2 Auto Scaling :
----------------------------
If you’ve tried to access a website that wouldn’t load and frequently timed out, the website might have received more requests than it was able to handle. This situation is similar to waiting in a long line at a coffee shop, when there is only one barista present to take orders from customers.


Amazon EC2 Auto Scaling enables you to automatically add or remove Amazon EC2 instances in response to changing application demand. By automatically scaling your instances in and out as needed, you can maintain a greater sense of application availability.

Within Amazon EC2 Auto Scaling, you can use two approaches: dynamic scaling and predictive scaling.

- Dynamic scaling responds to changing demand. 

- Predictive scaling automatically schedules the right number of Amazon EC2 instances based on predicted demand.

note : 1. To scale faster, you can use dynamic scaling and predictive scaling together.
-----        
       2. Amazon EC2 Auto Scaling enables you to automatically add or remove Amazon EC2 instances in response to changing application demand.




Elastic Load Balancing :
-------------------------
Elastic Load Balancing is the AWS service that automatically distributes incoming application traffic across multiple resources, such as Amazon EC2 instances.

A load balancer acts as a single point of contact for all incoming web traffic to your Auto Scaling group. This means that as you add or remove Amazon EC2 instances in response to the amount of incoming traffic, these requests route to the load balancer first. Then, the requests spread across multiple resources that will handle them. For example, if you have multiple Amazon EC2 instances, Elastic Load Balancing distributes the workload across the multiple instances so that no single instance has to carry the bulk of it.


Monolithic applications :
------------------------
Applications are made of multiple components. The components communicate with each other to transmit data, fulfill requests, and keep the application running. 

Suppose that you have an application with tightly coupled components. These components might include databases, servers, the user interface, business logic, and so on. This type of architecture can be considered a monolithic application. 

In this approach to application architecture, if a single component fails, other components fail, and possibly the entire application fails.


Microservices Approach :
------------------------
In a microservices approach, application components are loosely coupled. In this case, if a single component fails, the other components continue to work because they are communicating with each other. The loose coupling prevents the entire application from failing. 

When designing applications on AWS, you can take a microservices approach with services and components that fulfill different functions. Two services facilitate application integration: Amazon Simple Notification Service (Amazon SNS) and Amazon Simple Queue Service (Amazon SQS).



Amazon Simple Notification Service (Amazon SNS) :
------------------------------------------------
Amazon SNS is a publish/subscribe service. Using Amazon SNS topics, a publisher publishes messages to subscribers.



Amazon Simple Queue Service (Amazon SQS):    
-------------------------------------------
- Amazon Simple Queue Service (Amazon SQS) is a message queuing service. 

- Using Amazon SQS, you can send, store, and receive messages between software components, without losing messages or requiring other services to be available. In Amazon SQS, an application sends messages into a queue. A user or service retrieves a message from the queue, processes it, and then deletes it from the queue.


Container Orchestration tools of AWS :
---------------------------------------
1. Amazon Elastic Container Service

2. Amazon Elastic Kubernetes Service



Amazon EC2 Reserved Instances require a commitment of either 1 year or 3 years. The 3-year option offers a larger discount.


Amazon Elastic Kubernetes Service (Amazon EKS):
--------------------------------------------------
Amazon EKS is a fully managed Kubernetes service. Kubernetes is open-source software that enables you to deploy and manage containerized applications at scale.


AWS Lambda:
------------
AWS Lambda is a service that lets you run code without provisioning or managing servers.


Regions :
---------
Regions are geographically isolated areas where we can access services to run our enterprise.

Regions contains Availability zones.

An Availability Zone is a single data center or a group of data centers within a Region.


Edge Location :
---------------
An edge location is a site that Amazon CloudFront uses to store cached copies of your content closer to your customers for faster delivery.

Amazon CloudFront :
-------------------
A global content delivery service.  It uses a network of edge locations to cache content and deliver content to customers all over the world. 



Which action can you perform with AWS Outposts?
-------------------------------------------------
Extend AWS infrastructure and services to different locations including your on-premises data center.



Amazon Virtual Private Cloud (Amazon VPC) :
--------------------------------------------
A networking service that you can use to establish boundaries around your AWS resources.


Amazon VPC enables you to provision an isolated section of the AWS Cloud. In this isolated section, you can launch resources in a virtual network that you define. Within a virtual private cloud (VPC), you can organize your resources into subnets. A subnet is a section of a VPC that can contain resources such as Amazon EC2 instances.


Internet Gateway :
-------------------
An internet gateway is a connection between a VPC and the internet.

Without an internet gateway, no one can access the resources within your VPC.


To access private resources in a VPC, we can use a virtual private gateway. 

The virtual private gateway is the component that allows protected internet traffic to enter into the VPC.



AWS Direct Connect :
-----------------------
AWS Direct Connect is a service that lets you to establish a dedicated private connection between your data center and a VPC.  



AWS has wide range of tools that cover every layer of security.

1. Network Hardening 

2. Application Security

3. User identity

4. Authentication and Authorization

5. Distributed denial of service prevention

6. Data Integrity

7. Encryption



Subnets and Network Access Control Lists :
------------------------------------------

Subnets :
--------
A subnet is a section of a VPC in which we can group resources based on security or operational needs. Subnets can be public or private. 

Public subnets contain resources that need to be accessible by the public, such as an online store’s website.

Private subnets contain resources that should be accessible only through your private network, such as a database that contains customers’ personal information.


In a VPC, subnets can communicate with each other. For example, you might have an application that involves Amazon EC2 instances in a public subnet communicating with databases that are located in a private subnet.



Network traffic in a VPC :
-------------------------
When a customer requests data from an application hosted in the AWS Cloud, this request is sent as a packet. A packet is a unit of data sent over the internet or a network. 

It enters into a VPC through an internet gateway. Before a packet can enter into a subnet or exit from a subnet, it checks for permissions. These permissions indicate who sent the packet and how the packet is trying to communicate with the resources in a subnet.

The VPC component that checks packet permissions for subnets is a Network access control list. ( Network ACLs )

A network ACL is a virtual firewall that controls inbound and outbound traffic at the subnet level.


Security groups :
--------------------
A security group is a virtual firewall that controls inbound and outbound traffic for an Amazon EC2 instance.

By default, a security group denies all inbound traffic and allows all outbound traffic. You can add custom rules to configure which traffic should be allowed.

A packet travels over the internet from a client, to the internet gateway and into the VPC. Then the packet goes through the network access control list and accesses the public subnet, where two EC2 instances are located and has their security groups.


Amazon Route 53 Routing Policies :
-----------------------------------
- Latency-based routing
- Geolocation DNS
- Geoproximity routing
- Weighted round robin


Content Delivery Network: (CDN)
--------------------------
A network that delivers edge content to users based on their geographic location.


Domain Name System (DNS) :
--------------------------
Suppose that AnyCompany has a website hosted in the AWS Cloud. Customers enter the web address into their browser, and they are able to access the website. This happens because of Domain Name System (DNS) resolution. 

DNS resolution is the process of translating a domain name to an IP address. 


For example, suppose that you want to visit AnyCompany’s website. 

When you enter the domain name into your browser, this request is sent to a customer DNS resolver. 

The customer DNS resolver asks the company DNS server for the IP address that corresponds to AnyCompany’s website.

The company DNS server responds by providing the IP address for AnyCompany’s website, 192.0.2.0.



Amazon Route 53:
-----------------
Amazon Route 53 is a DNS web service. It gives developers and businesses a reliable way to route end users to internet applications hosted in AWS. 

Amazon Route 53 connects user requests to infrastructure running in AWS (such as Amazon EC2 instances and load balancers). It can route users to infrastructure outside of AWS.

Another feature of Route 53 is the ability to manage the DNS records for domain names. You can register new domain names directly in Route 53. You can also transfer DNS records for existing domain names managed by other domain registrars. This enables you to manage all of your domain names within a single location.


How Amazon Route 53 and Amazon CloudFront deliver content :
------------------------------------------------------------
Suppose that AnyCompany’s application is running on several Amazon EC2 instances. These instances are in an Auto Scaling group that attaches to an Application Load Balancer. 

A customer requests data from the application by going to AnyCompany’s website. 

Amazon Route 53 uses DNS resolution to identify AnyCompany.com’s corresponding IP address, 192.0.2.0. This information is sent back to the customer. 

The customer’s request is sent to the nearest edge location through Amazon CloudFront. 

Amazon CloudFront connects to the Application Load Balancer, which sends the incoming packet to an Amazon EC2 instance.



Instance stores:
------------------
Block-level storage volumes behave like physical hard drives.

An instance store provides temporary block-level storage for an Amazon EC2 instance. An instance store is disk storage that is physically attached to the host computer for an EC2 instance, and therefore has the same lifespan as the instance. When the instance is terminated, you lose any data in the instance store.

Amazon Elastic Block Store (Amazon EBS):
----------------------------------------
It is a service that provides block-level storage volumes that we can use with Amazon EC2 instances. If we stop or terminate an Amazon EC2 instance, all the data on the attached EBS volume remains available.

Because EBS volumes are for data that needs to persist, it’s important to back up the data. You can take incremental backups of EBS volumes by creating Amazon EBS snapshots.

An EBS snapshot(opens in a new tab) is an incremental backup. This means that the first backup taken of a volume copies all the data. For subsequent backups, only the blocks of data that have changed since the most recent snapshot are saved. 


In object storage, each object consists of data, metadata, and a key.

Amazon Simple Storage Service (Amazon S3):
--------------------------------------------
It is a service that provides object-level storage. Amazon S3 stores data as objects in buckets.

The maximum file size for an object in Amazon S3 is 5 TB.


Amazon S3 storage classes :
----------------------------
With Amazon S3, you pay only for what you use. You can choose from a range of storage classes to select a fit for your business and cost needs. 

When Selecting Amazon s3 storage class, consider these 2 factors ,

-> How often you plan to retrieve your data
-> How available you need your data to be


1. S3 Standard :
-----------------
-> Designed for frequently accessed data
-> Stores data in a minimum of three Availability Zones

Amazon S3 Standard provides high availability for objects. This makes it a good choice for a wide range of use cases, such as websites, content distribution, and data analytics. 

Amazon S3 Standard has a higher cost than other storage classes intended for infrequently accessed data and archival storage.


2. S3 Standard-Infrequent Access (S3 Standard-IA) :
----------------------------------------------------
-> Ideal for infrequently accessed data but requires high availability when needed.

-> Similar to Amazon S3 Standard but has a lower storage price and higher retrieval price.

-> It also stores data in a minimum of three Availability Zones.


3. S3 One Zone-Infrequent access (S3 One Zone-IA) :
----------------------------------------------------
-> Stores data in a single Availability Zone
-> Has a lower storage price than Amazon S3 Standard-IA

Compared to S3 Standard and S3 Standard-IA, which store data in a minimum of three Availability Zones, S3 One Zone-IA stores data in a single Availability Zone. This makes it a good storage class to consider if the following conditions apply:
    -> You want to save costs on storage.
    -> You can easily reproduce your data in the event of an Availability Zone failure.


4. S3 Intelligent-Tiering :
---------------------------
-> Ideal for data with unknown or changing access patterns.
-> Requires a small monthly monitoring and automation fee per object.

In the S3 Intelligent-Tiering storage class, Amazon S3 monitors objects’ access patterns. If you haven’t accessed an object for 30 consecutive days, Amazon S3 automatically moves it to the infrequent access tier, S3 Standard-IA. If you access an object in the infrequent access tier, Amazon S3 automatically moves it to the frequent access tier, S3 Standard.


5. S3 Glacier Instant Retrieval :
---------------------------------
-> Works well for archived data that requires immediate access.
-> Can retrieve objects within a few milliseconds.

When you decide between the options for archival storage, consider how quickly you must retrieve the archived objects. You can retrieve objects stored in the S3 Glacier Instant Retrieval storage class within milliseconds, with the same performance as S3 Standard.


6. S3 Glacier Flexible Retrieval :
----------------------------------
-> Low-cost storage designed for data archiving
-> Able to retrieve objects within a few minutes to hours

S3 Glacier Flexible Retrieval is a low-cost storage class that is ideal for data archiving. For example, you might use this storage class to store archived customer records or older photos and video files. You can retrieve your data from S3 Glacier Flexible Retrieval from 1 minute to 12 hours.


7. S3 Glacier Deep Archieve :
-------------------------------
-> Lowest-cost object storage class ideal for archiving
-> Able to retrieve objects within 12 hours

S3 Deep Archive supports long-term retention and digital preservation for data that might be accessed once or twice in a year. This storage class is the lowest-cost storage in the AWS Cloud, with data retrieval from 12 to 48 hours. All objects from this storage class are replicated and stored across at least three geographically dispersed Availability Zones.

8. S3 Outposts :
-----------------
-> Creates S3 buckets on Amazon S3 Outposts.
-> Makes it easier to retrieve, store, and access data on AWS Outposts.

Amazon S3 Outposts delivers object storage to your on-premises AWS Outposts environment. Amazon S3 Outposts is designed to store data durably and redundantly across multiple devices and servers on your Outposts. It works well for workloads with local data residency requirements that must satisfy demanding performance needs by keeping data close to on-premises applications.



Amazon Elastic File System (Amazon EFS) :
------------------------------------------
It is a scalable file system used with AWS Cloud services and on-premises resources. 
As you add and remove files, Amazon EFS grows and shrinks automatically. It can scale on demand to petabytes without disrupting applications. 


Comparing Amazon EBS and Amazon EFS :
-------------------------------------
An Amazon EBS volume stores data in a single Availability Zone. 
To attach an Amazon EC2 instance to an EBS volume, both the Amazon EC2 instance and the EBS volume must reside within the same Availability Zone.


Amazon EFS is a regional service. It stores data in and across multiple Availability Zones. 
The duplicate storage enables you to access data concurrently from all the Availability Zones in the Region where a file system is located. Additionally, on-premises servers can access Amazon EFS using AWS Direct Connect.


































